# Argus Multi-Agent Security Platform - Complete Codebase Analysis

**Date:** 2025-12-11
**Purpose:** Comprehensive analysis of actual implementation for code enhancement and debugging
**‚ö†Ô∏è IMPORTANT:** This document is based on actual code inspection, not documentation

---

## Table of Contents

1. [Executive Summary](#executive-summary)
2. [Critical Implementation Details](#critical-implementation-details)
3. [Agent Architecture - Actual Implementation](#agent-architecture---actual-implementation)
4. [Communication Layer](#communication-layer)
5. [Data Storage Layer](#data-storage-layer)
6. [Deployment Infrastructure](#deployment-infrastructure)
7. [UI Implementation](#ui-implementation)
8. [Known Issues and Limitations](#known-issues-and-limitations)
9. [Enhancement Opportunities](#enhancement-opportunities)

---

## Executive Summary

### What This System Actually Is

Argus is a **multi-agent security orchestration platform** built on Google's Agent Development Kit (ADK) that:
- Analyzes security threats using VirusTotal (via GTI MCP server)
- Provides incident response capabilities (simulated SOAR)
- Can be deployed as distributed microservices on Cloud Run
- Has a Streamlit-based chat interface for natural language interaction

### Technology Stack

| Component | Technology | Notes |
|-----------|-----------|-------|
| **Agent Framework** | Google ADK (Agent Development Kit) | Uses Gemini 2.0 Flash |
| **Threat Intelligence** | GTI MCP Server ‚Üí VirusTotal API | 35+ tools via MCP protocol |
| **Incident Response** | Simulated tools (in-memory) | NOT connected to real SOAR |
| **Communication** | FastAPI (A2A protocol over HTTPS) | Agent-to-Agent RPC |
| **Storage** | Google BigQuery (optional) | Fast-fails in non-GCP environments |
| **Deployment** | Google Cloud Run + Cloud Build | Docker containers |
| **UI** | Streamlit | Chat-first interface |
| **Package Management** | UV (Astral) | Modern Python dependency manager |

### Key Finding: Hybrid Architecture

**The agents use a hybrid approach - not pure LLM-based delegation:**
- Root Agent: **Manual routing** (regex parsing) ‚Üí Does NOT use ADK agent for routing
- Threat Agent: **Direct MCP tool calling** ‚Üí Bypasses LLM for reliability
- Incident Agent: **Mix of LLM and direct calls** ‚Üí Uses LLM for complex scenarios only

This is a **pragmatic design choice** for production reliability but differs from what pure ADK architecture might suggest.

---

## Critical Implementation Details

### What Actually Works vs. What's Documented

| Feature | Documented | Actual Implementation | Status |
|---------|-----------|----------------------|--------|
| **Vertex AI Agent Registry** | Agents register/discover via Vertex AI | Placeholder code - NOT implemented | ‚ö†Ô∏è Not Working |
| **Agent Discovery** | Via Vertex AI registry | Via .env.agents file + env vars | ‚úÖ Working |
| **Root Agent Routing** | LLM-based tool calling | Manual regex parsing + routing | ‚úÖ Working |
| **Threat Analysis** | ADK agent with MCP tools | Direct MCP tool calling (bypasses LLM) | ‚úÖ Working |
| **Incident Response** | Chronicle SOAR integration | Simulated in-memory tools | ‚ö†Ô∏è Demo Only |
| **BigQuery Storage** | Persistent memory for agents | Fast-fails in Streamlit Cloud | ‚ö†Ô∏è GCP Only |
| **A2A Endpoint Discovery** | Client resolves from registry | Requires explicit endpoint URLs | ‚ö†Ô∏è Manual Config |

---

## Agent Architecture - Actual Implementation

### Root Orchestrator Agent (`agents/root_agent.py`)

**Location:** `agents/root_agent.py` (895 lines)

#### What It Actually Does

The Root Agent is the **primary user-facing interface** but it does NOT use the ADK agent's natural language routing as you might expect. Instead:

**Actual Workflow (root_agent.py:549-630):**

```python
def chat(self, user_message: str) -> Dict[str, Any]:
    # 1. MANUAL PARSING - Uses regex to extract indicators
    indicators = self._extract_indicators(user_message)  # Line 583

    # 2. KEYWORD DETECTION - Not LLM-based
    is_block_action = any(word in message_lower for word in ['block', 'ban', 'blacklist'])
    is_isolate_action = any(word in message_lower for word in ['isolate', 'quarantine'])

    # 3. EXPLICIT ROUTING - No LLM decision making
    if is_block_action and indicators.get('ip'):
        action_result = self._call_incident_action("block_ip", indicators['ip'])
    elif indicators.get('ip'):
        analysis_result = self._call_threat_agent(indicators['ip'], 'ip')

    # 4. FORMAT RESPONSE - Pre-formatted, not LLM-generated
    response_text = self._format_response(user_message, analysis_result, action_result, timestamp)

    return {"text": response_text, "trace": trace}
```

**Key Point:** The ADK agent is initialized (line 167-172) but the `chat()` method **does not call `run_agent_sync()`**. It uses hardcoded logic instead.

#### Why This Design?

**Advantages:**
- ‚úÖ Predictable routing (no LLM hallucinations)
- ‚úÖ Fast response times (no LLM inference for routing)
- ‚úÖ Easier to debug and test
- ‚úÖ Lower cost (fewer API calls)

**Disadvantages:**
- ‚ùå Limited to predefined patterns (can't handle complex natural language)
- ‚ùå Requires regex maintenance for new indicator types
- ‚ùå Not leveraging full ADK capabilities

#### Indicator Extraction (root_agent.py:632-679)

```python
def _extract_indicators(self, text: str) -> Dict[str, Optional[str]]:
    indicators = {'ip': None, 'domain': None, 'hash': None, 'url': None, ...}

    # IP: \b(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})\b
    # URL: (https?://[^\s]+)
    # Hash: \b([a-fA-F0-9]{32}|[a-fA-F0-9]{40}|[a-fA-F0-9]{64})\b
    # Domain: \b([a-zA-Z0-9][-a-zA-Z0-9]*\.)+[a-zA-Z]{2,}\b

    return indicators
```

**Limitation:** Only extracts the FIRST match of each type. Multiple indicators in one message are not all processed.

#### Agent Discovery Priority (root_agent.py:332-404)

1. **Read .env.agents file** (lines 335-357) - Looks for `THREAT_AGENT_ENDPOINT`, `INCIDENT_AGENT_ENDPOINT`
2. **Vertex AI Registry** (lines 364, 382) - Calls `registry.discover_agent()` but it returns None (not implemented)
3. **Environment variables** (lines 370, 388) - Fallback to `THREAT_AGENT_ENDPOINT` env var
4. **Direct instantiation** (lines 436-440, 475-478) - Last resort, creates new agent instances

**Actual Discovery Method:** Option #1 (.env.agents file) is what actually works in practice.

#### Sub-Agent Delegation

**Three delegation methods:**

1. **`_call_threat_agent()`** (lines 406-443):
   - Tries A2A protocol first (if endpoint configured)
   - Falls back to pre-initialized instance (passed from UI)
   - Last resort: creates new instance (NOT recommended due to MCP timeout)

2. **`_call_incident_agent()`** (lines 445-481):
   - Same three-tier fallback pattern

3. **`_call_incident_action()`** (lines 483-517):
   - Quick actions bypass full incident handling

**Key Insight:** The UI passes pre-initialized sub-agents to Root Agent (ui.py:193-197) to avoid MCP re-initialization timeouts.

---

### Threat Analysis Agent (`agents/threat_agent.py`)

**Location:** `agents/threat_agent.py` (659 lines)

#### What It Actually Does

The Threat Agent connects to VirusTotal via the GTI MCP server, but it **does NOT use the ADK agent's LLM-based tool calling**. Instead, it calls MCP tools directly.

**Actual Workflow (threat_agent.py:501-569):**

```python
def analyze_indicator(self, indicator: str, indicator_type: str, context: str = "") -> dict:
    # 1. DIRECT TOOL MAPPING - No LLM decision
    tool_map = {
        "ip": ("get_ip_address_report", {"ip_address": indicator}),
        "domain": ("get_domain_report", {"domain": indicator}),
        "hash": ("get_file_report", {"hash": indicator}),
        "url": ("get_url_report", {"url": indicator}),
    }

    # 2. DIRECT MCP CALL - Bypasses ADK agent entirely
    tool_name, tool_args = tool_map[indicator_type]
    tool_result = self._call_tool_directly(tool_name, **tool_args)  # Line 533

    # 3. MANUAL PARSING - Custom response parsing
    analysis_result = self._parse_vt_response(indicator, indicator_type, raw_data)

    return {"success": True, "analysis": analysis_result, "mode": mode_info}
```

**Key Point:** The ADK agent is created (lines 307-319) but `analyze_indicator()` **does not call `run_agent_sync()`**. It's purely for potential future use or for the ADK Web UI.

#### Direct MCP Tool Calling (threat_agent.py:338-388)

```python
def _call_tool_directly(self, tool_name: str, **kwargs) -> dict:
    async def _call():
        # 1. Find gti_mcp binary (lines 344-354)
        gti_mcp_path = shutil.which('gti_mcp') or find_in_venv()

        # 2. Create MCP client directly (lines 357-362)
        server_params = StdioServerParameters(command=gti_mcp_path, ...)

        # 3. Call tool via MCP protocol (lines 363-368)
        async with stdio_client(server_params) as (read, write):
            async with ClientSession(read, write) as session:
                await session.initialize()
                result = await session.call_tool(tool_name, kwargs)

        return {"success": True, "data": parsed_json}

    return asyncio.run(_call())
```

**Why Direct Calling?**
- ‚úÖ **Reliability:** LLM might call wrong tools or malform parameters
- ‚úÖ **Speed:** No LLM inference overhead (saves ~2-3 seconds per request)
- ‚úÖ **Predictability:** Always calls the right tool for the indicator type
- ‚úÖ **Cost:** No extra Gemini API calls

#### MCP Tool Discovery (threat_agent.py:213-244)

```python
def _initialize_agent(self):
    # Create MCP toolset
    self.mcp_toolset = create_gti_mcp_toolset()  # Line 218

    if self.mcp_toolset:
        # Get tools with 10 SECOND TIMEOUT (Line 228)
        try:
            self.tools = asyncio.run(
                asyncio.wait_for(get_mcp_tools(self.mcp_toolset), timeout=10.0)
            )
            self.is_live_mode = len(self.tools) > 0
        except asyncio.TimeoutError:
            logger.warning("MCP tool discovery timed out after 10s - continuing in demo mode")
            self.tools = []
            self.is_live_mode = False
```

**Critical Design Choice:** 10-second timeout prevents hanging in Streamlit Cloud or other restricted environments.

#### VT Response Parsing (threat_agent.py:390-499)

**Custom severity calculation logic** (lines 413-429):

```python
malicious = last_analysis_stats.get('malicious', 0)
total = malicious + suspicious + harmless + undetected
detection_pct = (malicious / total) * 100 if total > 0 else 0

if malicious >= 10 or detection_pct >= 20:
    severity = "CRITICAL", confidence = 95
elif malicious >= 5 or detection_pct >= 10:
    severity = "HIGH", confidence = 85
elif malicious >= 2 or detection_pct >= 5 or reputation < -50:
    severity = "MEDIUM", confidence = 70
elif malicious >= 1 or suspicious >= 3:
    severity = "LOW", confidence = 55
else:
    severity = "INFO", confidence = 40
```

**Note:** These thresholds are hardcoded and may need tuning based on actual threat landscape.

#### GTI MCP Tools Available

When `VT_APIKEY` is configured and MCP connection succeeds, **35+ tools** are discovered (lines 246-261):

**Primary Analysis Tools:**
- `get_ip_address_report` - IP reputation and ownership
- `get_domain_report` - Domain analysis and categorization
- `get_file_report` - Malware analysis for hashes
- `get_url_report` - URL scanning and classification

**Related Entity Tools:**
- `get_entities_related_to_a_file` - Find related samples
- `get_entities_related_to_a_domain` - Find related infrastructure
- `get_entities_related_to_an_ip_address` - Find related IPs
- `get_entities_related_to_an_url` - Find related URLs

**Search and Intelligence:**
- `search_iocs` - Search indicators of compromise
- `search_threats` - Threat intelligence database search
- `search_malware_families` - Malware family lookup
- `search_threat_actors` - Attribution intelligence
- And 20+ more specialized tools...

**Current Usage:** Only the 4 primary analysis tools are actually used. The other 31+ tools are discovered but not leveraged.

---

### Incident Response Agent (`agents/incident_agent.py`)

**Location:** `agents/incident_agent.py` (507 lines)

#### What It Actually Does

The Incident Agent is the **ONLY agent that actually uses the ADK agent for LLM-based decision making**, but only in one method.

**Two Different Approaches:**

1. **`handle_incident()`** (lines 328-393) - **USES ADK Agent:**
   ```python
   def handle_incident(self, threat_analysis: dict, context: str = "") -> dict:
       # Build prompt for LLM
       incident_prompt = f"""A security threat has been identified...
       Threat Analysis: {json.dumps(threat_analysis)}
       Please execute the incident response workflow..."""

       # Execute via ADK agent (Line 364)
       content = run_agent_sync(self.agent, incident_prompt)

       return {"success": True, "response": content, "mode": mode_info}
   ```

2. **`execute_action()`** (lines 395-426) - **BYPASSES ADK Agent:**
   ```python
   def execute_action(self, action: str, target: str, case_id: str = "") -> dict:
       action_map = {
           "block_ip": lambda: block_ip(target, case_id),
           "isolate_endpoint": lambda: isolate_endpoint(target, case_id),
           "disable_user": lambda: disable_user(target, case_id),
       }

       # Direct tool call (Line 415)
       result = json.loads(action_map[action]())

       return {"success": True, "result": result, "mode": mode_info}
   ```

**Design Rationale:**
- Complex incident handling ‚Üí Use LLM for decision making
- Simple quick actions ‚Üí Direct execution for speed

#### SOAR Tools - All Simulated (lines 29-199)

**‚ö†Ô∏è CRITICAL:** No real SOAR integration exists. All tools are simulated:

```python
# In-memory case storage (Lines 33-35)
_cases: Dict[str, Dict] = {}
_case_counter = [0]

# Check if real SOAR configured (Lines 37-39)
SOAR_API_KEY = os.getenv("SOAR_API_KEY", "")
IS_LIVE_SOAR = bool(SOAR_API_KEY and not SOAR_API_KEY.startswith("your-"))
# Result: IS_LIVE_SOAR = False (always, since no real API key)
```

**Simulated Tools:**

1. **`create_case()`** (lines 42-73):
   - Generates case ID: `CASE-YYYYMMDD-NNNN`
   - Stores in `_cases` dict (in-memory only)
   - Returns: `{"case_id": "...", "status": "Open", "source": "Simulated SOAR"}`

2. **`block_ip()`** (lines 76-105):
   - Logs action (line 90)
   - Returns: `{"action": "block_ip", "status": "SUCCESS", "message": "[SIMULATED] IP ... blocked"}`
   - **Does NOT actually block anything**

3. **`isolate_endpoint()`** (lines 108-137):
   - Returns: `{"message": "[SIMULATED] Endpoint ... isolated from network"}`
   - **Does NOT actually isolate anything**

4. **`disable_user()`** (lines 140-169):
   - Returns: `{"message": "[SIMULATED] User account ... disabled and sessions revoked"}`
   - **Does NOT actually disable anything**

**Visibility:** All responses include `"source": "Simulated SOAR"` to indicate demo mode.

#### Mode Indicator (lines 318-326)

```python
def get_mode_indicator(self) -> Dict[str, Any]:
    return {
        "is_live": self.is_live_mode,  # Always False
        "mode": "Live" if self.is_live_mode else "Demo",
        "source": "Chronicle SOAR" if self.is_live_mode else "Simulated SOAR",
        "tools_count": len(self.tools),  # 6 simulated tools
        "icon": "üü¢" if self.is_live_mode else "üü°"  # Always üü°
    }
```

---

## Communication Layer

### A2A (Agent-to-Agent) Protocol

**Implementation:** `shared/communication/a2a_server_fastapi.py` (229 lines)

#### What A2A Actually Is

A simple HTTP/JSON RPC protocol for invoking agent methods remotely:

**Request Format:**
```json
POST /a2a/invoke
{
  "agent": "ThreatAnalysisAgent",
  "method": "analyze_indicator",
  "params": {"indicator": "8.8.8.8", "indicator_type": "ip"},
  "protocol_version": "1.0"
}
```

**Response Format:**
```json
{
  "success": true,
  "agent": "ThreatAnalysisAgent",
  "method": "analyze_indicator",
  "result": {...}
}
```

#### A2A Server Implementation (a2a_server_fastapi.py)

**Key Routes:**

1. **`GET /health`** (lines 44-46) - Health check, responds immediately
2. **`GET /`** (lines 49-62) - Service info and endpoint list
3. **`POST /a2a/invoke`** (lines 64-218) - **Main RPC endpoint**

**Method Invocation Logic** (lines 174-202):

```python
# Execute method directly
handler = self.methods[request_data.method]
sig = inspect.signature(handler)
params_list = list(sig.parameters.keys())

# Smart parameter passing (Lines 181-202)
if len(params_list) == 1:
    # Single dict parameter (e.g., threat_analysis)
    if is_dict_param and isinstance(request_data.params, dict):
        result = handler(request_data.params)  # Pass dict directly
    else:
        result = handler(**request_data.params)  # Unpack as kwargs
else:
    # Multiple parameters
    result = handler(**request_data.params)  # Unpack as kwargs
```

**This smart parameter handling prevents issues with** single dict parameters being unpacked incorrectly.

#### A2A Client Implementation (a2a_client.py)

**Critical Finding:** The client's agent discovery is **NOT implemented** (lines 80-93):

```python
def _resolve_agent_endpoint(self, agent_name: str) -> Optional[str]:
    """Resolve agent endpoint from Vertex AI Agent Registry"""
    # TODO: Implement actual Vertex AI Agent Registry lookup
    logger.warning(f"Agent endpoint resolution not fully implemented for {agent_name}")
    return None  # Always returns None!
```

**Implication:** All A2A calls **MUST** provide explicit `endpoint` parameter. The client cannot auto-discover agents.

**Authentication** (lines 21-29):

```python
def _get_auth_headers(self) -> Dict[str, str]:
    if self.credentials:
        self.credentials.refresh(Request())
        return {
            "Authorization": f"Bearer {self.credentials.token}",
            "Content-Type": "application/json"
        }
    return {"Content-Type": "application/json"}
```

Uses Google Cloud default credentials with bearer token authentication.

---

### Vertex AI Agent Registry

**Implementation:** `shared/discovery/vertex_registry.py` (126 lines)

**‚ö†Ô∏è CRITICAL FINDING:** This is **placeholder code only**. None of the registry operations are actually implemented.

#### `register_agent()` (lines 20-62)

```python
def register_agent(self, agent_name: str, endpoint: str, capabilities: List[str], ...):
    # Create agent resource
    agent_resource = {
        "name": agent_name,
        "endpoint": endpoint,
        "capabilities": capabilities,
        "status": "ACTIVE"
    }

    # In production, this would create an actual Vertex AI resource
    # For now, we'll log and return a placeholder
    logger.info(f"Registered agent {agent_name} with endpoint {endpoint}")

    # Return fake resource name
    return f"projects/{self.project_id}/locations/{self.location}/agents/{agent_name}"
```

**What it does:** Logs the registration, returns a fake resource name. **Does NOT actually register anything.**

#### `discover_agent()` (lines 64-82)

```python
def discover_agent(self, agent_name: str) -> Optional[Dict[str, Any]]:
    # In production, this would query Vertex AI Agent Registry
    # For now, return None
    logger.warning(f"Agent discovery for {agent_name} not fully implemented")
    return None  # Always returns None!
```

**What it does:** Always returns `None`. **Does NOT actually discover anything.**

#### `list_agents()` (lines 84-102)

```python
def list_agents(self, filter_capabilities: Optional[List[str]] = None):
    # In production, this would query Vertex AI Agent Registry
    # For now, return empty list
    logger.warning("Agent listing not fully implemented")
    return []  # Always returns empty list!
```

**Actual Agent Discovery Mechanism:**

Since the registry doesn't work, the **actual** discovery happens via:

1. **.env.agents file** - Written by deployment scripts (root_agent.py:335-357)
   ```
   THREAT_AGENT_ENDPOINT=https://threat-analysis-agent-xxx-uc.a.run.app
   INCIDENT_AGENT_ENDPOINT=https://incident-response-agent-xxx-uc.a.run.app
   ```

2. **Environment variables** - Set manually or in .env file

3. **Pre-initialized instances** - Passed from UI to Root Agent (ui.py:193-197)

---

## Data Storage Layer

### BigQuery Integration

**Implementations:**
- `shared/memory/threat_memory.py` (119 lines)
- `shared/memory/incident_memory.py` (111 lines)

#### Environment Detection (Both files, lines 13-21)

```python
def is_gcp_environment() -> bool:
    """Quick check if running in GCP (has credentials configured)"""
    return (
        os.getenv("GOOGLE_APPLICATION_CREDENTIALS") is not None or
        os.getenv("GOOGLE_CLOUD_PROJECT") is not None and
        os.path.exists("/var/run/secrets/kubernetes.io") or  # GKE
        os.getenv("K_SERVICE") is not None  # Cloud Run
    )
```

#### Fast-Fail Design (threat_memory.py:27-30)

```python
class ThreatIntelMemory:
    def __init__(self, project_id: str):
        # Fast-fail if not in GCP environment (avoids 3s timeout)
        if not is_gcp_environment():
            raise RuntimeError("Not in GCP environment - BigQuery unavailable (this is expected in Streamlit Cloud)")
```

**Purpose:** Prevents 3-second BigQuery connection timeout in Streamlit Cloud.

**Agent Handling:** All agents have try/except around memory initialization (e.g., root_agent.py:149-161):

```python
try:
    self.threat_memory = ThreatIntelMemory(project_id)
    logger.info("Threat intelligence memory initialized")
except Exception as e:
    logger.warning(f"Memory not available: {e}")
    self.threat_memory = None  # Continue without persistence
```

#### BigQuery Schema

**Dataset:** `security_intel`

**Table 1: `threat_intelligence`** (threat_memory.py:34-36)

Inferred schema from storage code (lines 38-57):
```sql
CREATE TABLE threat_intelligence (
  indicator STRING,
  indicator_type STRING,
  threat_type STRING,
  severity STRING,
  confidence INT64,
  source STRING,
  mitre_techniques STRING,  -- or ARRAY<STRING>
  first_seen TIMESTAMP,
  last_seen TIMESTAMP,
  analyzed_at TIMESTAMP,
  agent STRING
)
```

**Table 2: `active_incidents`** (incident_memory.py:34-35)

Inferred schema from storage code (lines 37-55):
```sql
CREATE TABLE active_incidents (
  incident_id STRING,
  threat_indicator STRING,
  indicator_type STRING,
  severity STRING,
  status STRING,  -- OPEN, IN_PROGRESS, INVESTIGATING, RESOLVED
  response_summary STRING,
  created_at TIMESTAMP,
  updated_at TIMESTAMP,
  source STRING
)
```

**Note:** These tables must be created manually before agents can use BigQuery. No auto-creation logic exists.

---

## Deployment Infrastructure

### Docker Images

**Three separate containers** (one per agent):

1. **`Dockerfile.root_agent`** (44 lines)
2. **`Dockerfile.threat_agent`** (42 lines)
3. **`Dockerfile.incident_agent`** (42 lines)

#### Common Pattern (All Dockerfiles)

```dockerfile
FROM python:3.11-slim

# Install UV package manager
COPY --from=ghcr.io/astral-sh/uv:latest /uv /usr/local/bin/uv

# Install dependencies
COPY pyproject.toml uv.lock* ./
RUN uv sync --frozen --no-dev

# Make uv-installed packages available
ENV PATH="/app/.venv/bin:$PATH"

# Copy code (only what's needed for this agent)
COPY shared/ ./shared/
COPY agents/threat_agent.py ./agents/

# Create ADK web UI structure (Lines 23-25 in threat_agent)
RUN mkdir -p adk_web_ui/threat_agent && \
    echo '"""Threat Analysis Agent for ADK Web UI"""' > adk_web_ui/threat_agent/__init__.py && \
    printf 'import os\nimport sys\nsys.path.insert(0, '\''/app'\'')\n...\nroot_agent = threat_agent.agent\n' > adk_web_ui/threat_agent/agent.py

# Run agent server
CMD ["/app/.venv/bin/python", "-m", "agents.threat_agent"]
```

**ADK Web UI Structure:** Each container creates a special directory structure (`adk_web_ui/{agent_name}/agent.py`) that exposes the ADK agent for monitoring via `/web` endpoint.

### Deployment Scripts

**Pattern: Cloud Build + Cloud Run** (deploy_threat_agent.sh as example)

#### Step 1: Load Environment (Lines 12-25)

```bash
if [ -f "${PROJECT_ROOT}/.env" ]; then
    source "${PROJECT_ROOT}/.env"
elif [ -f "${PROJECT_ROOT}/cloud_dev.env" ]; then
    source "${PROJECT_ROOT}/cloud_dev.env"
fi
```

Loads credentials and configuration from `.env` or `cloud_dev.env`.

#### Step 2: Build with Cloud Build (Lines 44-61)

```bash
# Create temporary cloudbuild.yaml
cat > /tmp/cloudbuild-threat-agent.yaml <<EOF
steps:
- name: 'gcr.io/cloud-builders/docker'
  args: ['build', '-t', '${IMAGE_NAME}', '-f', 'deployment/Dockerfile.threat_agent', '.']
images:
- '${IMAGE_NAME}'
EOF

gcloud builds submit --config /tmp/cloudbuild-threat-agent.yaml --project ${PROJECT_ID} .
```

**Key Benefit:** No local Docker required. Cloud Build handles the build.

#### Step 3: Deploy to Cloud Run (Lines 64-76)

```bash
gcloud run deploy threat-analysis-agent \
  --image gcr.io/${PROJECT_ID}/threat-analysis-agent \
  --platform managed \
  --region us-central1 \
  --allow-unauthenticated \
  --set-env-vars GOOGLE_CLOUD_PROJECT=${PROJECT_ID} \
  --set-env-vars GOOGLE_API_KEY=${GOOGLE_API_KEY} \
  --set-env-vars VT_APIKEY=${VT_APIKEY} \
  --memory 2Gi \
  --cpu 2 \
  --timeout 300
```

**Configuration:**
- 2Gi memory, 2 CPU (generous for MCP + LLM)
- 300s timeout (5 minutes for long-running analyses)
- Unauthenticated access (protected by obscure URL)

#### Step 4: Write Endpoint to .env.agents (Lines 88-96)

```bash
ENDPOINT=$(gcloud run services describe threat-analysis-agent --format 'value(status.url)')

# Write to .env.agents file
echo "THREAT_AGENT_ENDPOINT=${ENDPOINT}" >> .env.agents
```

**This is the actual discovery mechanism** - not Vertex AI registry.

#### Step 5: "Register" in Vertex AI (Lines 99-123)

```bash
python3 -c "
from shared.vertex_registry import VertexAIAgentRegistry
registry = VertexAIAgentRegistry('${PROJECT_ID}', '${LOCATION}')
registry.register_agent(
    agent_name='ThreatAnalysisAgent',
    endpoint='${ENDPOINT}',
    capabilities=['analyze_indicator', 'threat_intelligence']
)
" 2>/dev/null || echo "‚ö† Warning: Could not register (this is optional)"
```

**Note:** This "registration" only logs the agent (see vertex_registry.py analysis above). It does NOT actually register anything.

### Deployment Order

**IMPORTANT:** Sub-agents must be deployed before the root agent:

```bash
# 1. Deploy sub-agents first (they write endpoints to .env.agents)
./deployment/deploy_threat_agent.sh
./deployment/deploy_incident_agent.sh

# 2. Deploy root agent (it reads .env.agents to discover sub-agents)
./deployment/deploy_root_agent.sh

# 3. Deploy UI to Streamlit Cloud (manual - via Streamlit dashboard)
```

---

## UI Implementation

**Location:** `ui.py` (full file, Streamlit app)

### Agent Initialization (Lines 175-208)

```python
@st.cache_resource
def get_agents():
    """Initialize agents (cached to avoid re-initialization)"""
    project_id = os.getenv("GOOGLE_CLOUD_PROJECT", "demo-project")

    try:
        # Initialize sub-agents FIRST (Line 186)
        threat_agent = ThreatAnalysisAgent(project_id)  # Has 10s MCP timeout
        incident_agent = IncidentResponseAgent(project_id)

        # Pass sub-agents to Root Agent (Line 193-197)
        root_agent = RootOrchestratorAgent(
            project_id,
            threat_agent=threat_agent,  # Pre-initialized!
            incident_agent=incident_agent  # Pre-initialized!
        )

        return {"root": root_agent, "threat": threat_agent, "incident": incident_agent}
    except Exception as e:
        logging.error(f"Failed to initialize agents: {e}", exc_info=True)
        return None
```

**Critical Design:** Sub-agents are initialized first and passed to Root Agent. This prevents:
- ‚ùå MCP re-initialization timeout (10s timeout would happen twice)
- ‚ùå Duplicate tool discovery
- ‚ùå Multiple BigQuery clients

### Streamlit Cloud Secrets (Lines 17-25)

```python
try:
    if hasattr(st, 'secrets'):
        for key in ['VT_APIKEY', 'GOOGLE_API_KEY', 'GOOGLE_CLOUD_PROJECT']:
            if key in st.secrets:
                os.environ[key] = st.secrets[key]
except Exception:
    pass  # Running locally with .env
```

Loads secrets from Streamlit Cloud dashboard (not committed to repo).

### Mode Detection (Lines 220-236)

```python
if st.session_state.mode_info is None:
    threat_mode = agents["threat"].get_mode_indicator()
    incident_mode = agents["incident"].get_mode_indicator()
    st.session_state.mode_info = {
        "threat": threat_mode,
        "incident": incident_mode,
        "overall_live": threat_mode.get("is_live", False)  # Only threat agent can be live
    }
```

**Displays in UI:**
- üü¢ **Live Mode** - VT_APIKEY configured, GTI MCP tools discovered
- üü° **Demo Mode** - No VT_APIKEY or MCP connection failed

### Professional UI Styling (Lines 242-300)

Custom CSS with dark theme:
- **Font:** Inter (UI), JetBrains Mono (code)
- **Colors:** Dark background (#0e0e11), accent (#6366f1 - indigo)
- **Severity Colors:** Red (Critical), Orange (High), Yellow (Medium), Green (Low)

### Tab Structure (Not shown in excerpt, but present in full file)

1. **Argus Chat** (Default) - Natural language interface ‚Üí calls `root_agent.chat()`
2. **Threat Intel** - Direct threat analysis ‚Üí calls `threat_agent.analyze_indicator()`
3. **Incident Response** - SOAR actions ‚Üí calls `incident_agent.execute_action()`
4. **Activity Log** - Session history from `st.session_state.messages`

---

## Known Issues and Limitations

### 1. Vertex AI Agent Registry - Not Implemented

**Files Affected:**
- `shared/discovery/vertex_registry.py`
- `shared/communication/a2a_client.py:80-93`

**Issue:** All registry operations (`register_agent()`, `discover_agent()`, `list_agents()`) are placeholder code that just logs and returns None/empty.

**Impact:**
- ‚ùå Agents cannot auto-discover each other
- ‚ùå Must manually configure endpoints in .env.agents
- ‚ùå No dynamic service discovery

**Workaround:** Use .env.agents file (current approach).

**To Fix:** Implement actual Vertex AI Agent Registry API integration or use alternative service discovery (e.g., Cloud Run service catalog, Consul, etcd).

---

### 2. Root Agent - Limited Natural Language Understanding

**File:** `agents/root_agent.py:549-630`

**Issue:** The `chat()` method uses regex-based routing instead of LLM-based understanding.

**Limitations:**
- ‚ùå Only recognizes predefined patterns (IP, domain, hash, URL)
- ‚ùå Cannot handle complex queries like "Show me all high-severity threats from today"
- ‚ùå Extracts only FIRST indicator of each type (multi-indicator queries fail)
- ‚ùå Keyword detection is naive (`block` matches "unblock" too)

**Example Failures:**
- "Analyze 1.2.3.4 and 5.6.7.8" ‚Üí Only analyzes 1.2.3.4
- "Has evil.com contacted 10.0.0.1?" ‚Üí Might analyze domain OR IP, not relationship
- "Block 1.2.3.4 and 5.6.7.8" ‚Üí Only blocks 1.2.3.4

**Why This Design?**
- ‚úÖ Predictable and testable
- ‚úÖ Fast (no LLM routing overhead)
- ‚úÖ Lower cost

**To Fix:**
- **Option A:** Use the ADK agent properly - call `run_agent_sync(self.agent, user_message)` instead of manual routing
- **Option B:** Enhance regex to extract ALL indicators, not just first match
- **Option C:** Add LLM fallback for complex queries

---

### 3. Threat Agent - Unused MCP Tools

**File:** `agents/threat_agent.py:246-261`

**Issue:** GTI MCP server provides 35+ tools, but only 4 are used:
- ‚úÖ Used: `get_ip_address_report`, `get_domain_report`, `get_file_report`, `get_url_report`
- ‚ùå Unused: `search_iocs`, `search_threats`, `search_malware_families`, `get_entities_related_to_*` (31+ more)

**Missed Capabilities:**
- Threat hunting (search_iocs, search_threats)
- Malware family attribution (search_malware_families)
- Threat actor intelligence (search_threat_actors)
- Related entity pivoting (get_entities_related_to_*)
- MITRE ATT&CK mapping tools
- Historical analysis tools

**Why Unused?**
- Direct tool calling approach (`_call_tool_directly()`) only maps 4 tools
- No LLM decision-making to choose additional tools
- No support for multi-step analysis workflows

**To Fix:**
- Add more tool mappings to `analyze_indicator()`
- Implement pivot analysis (e.g., "find all domains hosted on this IP")
- Create specialized methods for threat hunting, attribution, etc.

---

### 4. Incident Agent - No Real SOAR Integration

**File:** `agents/incident_agent.py:29-199`

**Issue:** All SOAR tools are simulated with in-memory dictionaries.

**What Doesn't Work:**
- ‚ùå `block_ip()` - Doesn't actually block anything
- ‚ùå `isolate_endpoint()` - Doesn't actually isolate anything
- ‚ùå `disable_user()` - Doesn't actually disable anything
- ‚ùå Cases stored in `_cases` dict - Lost on restart
- ‚ùå No integration with real firewalls, EDR, IAM, etc.

**Visibility:** All responses include `"source": "Simulated SOAR"` but this might not be obvious to users.

**To Fix:**
- Integrate with Chronicle SOAR MCP server (when available)
- Implement direct API calls to security tools (Palo Alto, CrowdStrike, Okta, etc.)
- Add configuration for which tools to use (user choice)
- Make demo mode MORE obvious in UI

---

### 5. BigQuery - Doesn't Work in Streamlit Cloud

**Files:** `shared/memory/threat_memory.py`, `shared/memory/incident_memory.py`

**Issue:** Fast-fail detection prevents BigQuery in non-GCP environments.

**Current Behavior:**
- ‚úÖ Cloud Run deployment: BigQuery works (K_SERVICE env var detected)
- ‚ùå Streamlit Cloud: BigQuery disabled (not GCP environment)
- ‚úÖ Local with credentials: BigQuery works (GOOGLE_APPLICATION_CREDENTIALS set)

**Impact:**
- No persistence in Streamlit Cloud demo
- All analysis history lost on session timeout
- No cross-session correlation

**To Fix:**
- **Option A:** Use Streamlit Cloud's secrets for GCP credentials (might work)
- **Option B:** Add alternative storage backend (PostgreSQL, MongoDB, etc.)
- **Option C:** Implement local SQLite fallback for demos
- **Option D:** Store in `st.session_state` for within-session persistence (current implicit behavior)

---

### 6. MCP Connection Timeout - Can Cause UI Hangs

**File:** `agents/threat_agent.py:224-239`

**Issue:** MCP tool discovery has 10-second timeout, which can feel slow in UI.

**Current Mitigation:**
- Streamlit caching (`@st.cache_resource`) prevents re-initialization
- 10s timeout prevents indefinite hangs
- Pre-initialization in UI prevents double timeout

**Remaining Issues:**
- First load in Streamlit Cloud takes ~10s (bad UX)
- Timeout errors not surfaced to UI clearly
- Demo mode fallback is silent

**To Fix:**
- Add loading spinner with status messages ("Connecting to threat intelligence...")
- Show clearer error messages if MCP fails
- Implement async MCP initialization (don't block UI)
- Add "Retry" button if connection fails

---

### 7. No Multi-Indicator Analysis

**File:** `agents/root_agent.py:632-679`

**Issue:** `_extract_indicators()` only extracts FIRST match of each type.

**Example:**
```python
message = "Analyze 1.2.3.4, 5.6.7.8, and 9.10.11.12"
indicators = self._extract_indicators(message)
# Result: indicators['ip'] = "1.2.3.4" (others ignored!)
```

**Impact:**
- Bulk analysis requests fail
- "Compare X and Y" queries don't work
- Investigation workflow broken for multi-IOC incidents

**To Fix:**
```python
def _extract_indicators(self, text: str) -> Dict[str, List[str]]:
    indicators = {
        'ips': [],  # Changed from 'ip' to 'ips'
        'domains': [],
        'hashes': [],
        'urls': []
    }

    # Find ALL matches
    indicators['ips'] = re.findall(r'\b(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})\b', text)
    indicators['hashes'] = re.findall(r'\b([a-fA-F0-9]{32}|[a-fA-F0-9]{40}|[a-fA-F0-9]{64})\b', text)
    # etc...

    return indicators
```

Then update routing logic to iterate over lists.

---

### 8. No Error Recovery in A2A Communication

**File:** `shared/communication/a2a_client.py:66-78`

**Issue:** A2A client has no retry logic or circuit breaker.

**Current Behavior:**
```python
response = requests.post(f"{endpoint}/a2a/invoke", json=a2a_request, headers=headers, timeout=30)
response.raise_for_status()  # Raises exception on error
return response.json()
```

**Problems:**
- Single network error kills the request
- No retry for transient failures
- 30s timeout might be too long (or too short for complex operations)
- No fallback if sub-agent is down

**To Fix:**
- Add retry with exponential backoff (using `tenacity` library)
- Implement circuit breaker pattern (using `pybreaker`)
- Add health check before calling (use `/health` endpoint)
- Better error messages for debugging

---

### 9. ADK Agents Created But Not Used

**Files:** All three agent files

**Issue:** ADK agents are initialized but:
- Root Agent: Doesn't use its ADK agent for routing
- Threat Agent: Doesn't use its ADK agent for analysis
- Incident Agent: Only uses ADK agent for `handle_incident()`, not `execute_action()`

**Why?**
- Reliability: Direct calling is more predictable
- Speed: No LLM inference overhead
- Cost: Fewer Gemini API calls

**But:**
- Not leveraging ADK's full capabilities (multi-turn, context, memory)
- ADK Web UI shows incomplete traces (since agents aren't fully using Runner)
- System prompts go unused

**To Consider:**
- Are you building an "ADK multi-agent system" or a "microservices system with ADK components"?
- If the former, refactor to use ADK properly
- If the latter, remove unused ADK setup to reduce confusion

---

### 10. No Authentication/Authorization

**Files:** Deployment scripts (`--allow-unauthenticated`)

**Issue:** Cloud Run services are deployed with unauthenticated access.

**Current Security:**
- ‚úÖ Obscure URLs (hard to guess)
- ‚ùå No authentication required
- ‚ùå No rate limiting
- ‚ùå No API key required
- ‚ùå Anyone with URL can use agents

**Risk Level:**
- Medium: VirusTotal API key could be exhausted by unauthorized users
- Medium: Gemini API costs could increase from abuse
- Low: Simulated SOAR can't cause real damage

**To Fix:**
- Add Cloud IAM authentication (remove `--allow-unauthenticated`)
- Implement API key authentication at A2A layer
- Add rate limiting (Cloud Armor, or application-level)
- Monitor usage and set billing alerts

---

## Enhancement Opportunities

### High Impact, Low Effort

1. **Better Multi-Indicator Support** (2-4 hours)
   - Change `_extract_indicators()` to return lists
   - Update routing logic to iterate over all indicators
   - Show combined analysis results in UI

2. **Clearer Demo Mode Indicators** (1-2 hours)
   - Add prominent banner when in demo mode
   - Prefix all simulated action results with "üü° [SIMULATED]"
   - Add mode indicator to every response
   - Explain what's real vs. simulated in UI

3. **MCP Connection Feedback** (2-3 hours)
   - Add Streamlit progress bar during MCP initialization
   - Show status messages ("Connecting to VirusTotal...", "Discovered 35 tools")
   - Surface timeout errors more clearly
   - Add "Retry Connection" button

4. **Leverage More GTI MCP Tools** (4-6 hours)
   - Add `search_related_files()` method to Threat Agent
   - Implement "pivot analysis" (from IP ‚Üí find domains, from domain ‚Üí find files)
   - Add threat hunting workflows using search tools
   - Use MITRE ATT&CK mapping tools

5. **Fix A2A Client Error Handling** (2-3 hours)
   - Add retry with exponential backoff (use `tenacity`)
   - Implement health checks before calling
   - Better error messages and logging
   - Add timeout configuration per endpoint

### High Impact, Medium Effort

6. **Implement Actual Vertex AI Agent Registry** (1-2 days)
   - Research Vertex AI Agent Registry API (if available)
   - Implement register/discover/list operations
   - Update deployment scripts to use real registration
   - Remove .env.agents workaround

7. **Add LLM Routing to Root Agent** (1-2 days)
   - Refactor `chat()` to use `run_agent_sync(self.agent, message)`
   - Remove hardcoded regex routing
   - Test with complex queries
   - Keep regex as fallback for speed

8. **Persistent Storage Fallback** (2-3 days)
   - Detect environment (GCP vs. Streamlit Cloud vs. Local)
   - Implement SQLite backend for non-GCP environments
   - Add storage abstraction layer
   - Migrate from BigQuery-specific code

9. **Real SOAR Integration** (3-5 days)
   - Choose integration targets (Chronicle SOAR, PagerDuty, Slack, etc.)
   - Implement API clients for chosen tools
   - Add configuration UI for credentials
   - Keep simulated mode as demo option

10. **Enhanced UI Features** (3-5 days)
    - Add bulk analysis (upload CSV of indicators)
    - Show historical analysis from BigQuery (if available)
    - Add export functionality (JSON, CSV, PDF report)
    - Implement real-time monitoring dashboard
    - Add comparison view (analyze multiple indicators side-by-side)

### High Impact, High Effort

11. **Complete ADK Integration** (1-2 weeks)
    - Refactor all agents to properly use ADK Runner
    - Implement multi-turn conversations
    - Use ADK memory for context across turns
    - Full ADK Web UI support with traces
    - Evaluation framework for ADK agents

12. **Multi-Agent Orchestration** (1-2 weeks)
    - Implement coordination workflows (threat hunt ‚Üí response ‚Üí documentation)
    - Add agent-to-agent communication patterns (not just Root ‚Üí Sub)
    - Implement parallel analysis (multiple threat agents for different IOCs)
    - Add workflow engine (temporal.io or similar)

13. **Production Security Hardening** (1-2 weeks)
    - Implement Cloud IAM authentication
    - Add API key management
    - Rate limiting and quota enforcement
    - Audit logging (Cloud Logging)
    - Secret management (Secret Manager, not env vars)
    - Network security (VPC, Cloud Armor)

14. **Comprehensive Evaluation** (1-2 weeks)
    - Implement evaluation framework from EVALUATION_FRAMEWORK.md
    - Create test suites for each agent
    - Benchmark response quality
    - Measure latency and cost
    - Compare LLM-based vs. direct calling approaches

---

## File Reference Index

**For quick navigation during development:**

### Core Agents
- **Root Agent:** `agents/root_agent.py` (895 lines)
  - Key Methods: `chat()` (549), `_extract_indicators()` (632), `_call_threat_agent()` (406)
- **Threat Agent:** `agents/threat_agent.py` (659 lines)
  - Key Methods: `analyze_indicator()` (501), `_call_tool_directly()` (338), `_parse_vt_response()` (390)
- **Incident Agent:** `agents/incident_agent.py` (507 lines)
  - Key Methods: `handle_incident()` (328), `execute_action()` (395)
  - SOAR Tools: `create_case()` (42), `block_ip()` (76), `isolate_endpoint()` (108), `disable_user()` (140)

### Communication
- **A2A Client:** `shared/communication/a2a_client.py` (110 lines)
  - Issues: `_resolve_agent_endpoint()` (80) - NOT implemented
- **A2A Server:** `shared/communication/a2a_server_fastapi.py` (229 lines)
  - Key Route: `/a2a/invoke` (64)
- **Vertex Registry:** `shared/discovery/vertex_registry.py` (126 lines)
  - Issues: ALL methods are placeholders (register_agent, discover_agent, list_agents)

### Storage
- **Threat Memory:** `shared/memory/threat_memory.py` (119 lines)
  - Fast-fail: `is_gcp_environment()` (13), `__init__()` (27)
- **Incident Memory:** `shared/memory/incident_memory.py` (111 lines)
  - Same fast-fail pattern

### Deployment
- **Threat Agent Deployment:** `deployment/deploy_threat_agent.sh` (129 lines)
  - Cloud Build: (47), Cloud Run: (65), .env.agents: (88)
- **Dockerfile:** `deployment/Dockerfile.threat_agent` (42 lines)
  - ADK Web UI structure: (23)

### UI
- **Streamlit UI:** `ui.py` (full file)
  - Agent Init: `get_agents()` (175), Mode Detection: (220)
  - Streamlit Secrets: (17)

---

## Summary: What Actually Works

### ‚úÖ Working Features

1. **Threat Analysis (Live Mode)**
   - Real VirusTotal analysis when VT_APIKEY configured
   - Accurate severity assessment with confidence scores
   - IP, domain, hash, URL support
   - 10s timeout protection prevents hangs

2. **Manual Routing (Root Agent)**
   - Regex-based indicator extraction
   - Keyword-based action detection
   - Predictable and fast routing
   - No LLM hallucinations

3. **A2A Communication**
   - HTTP/JSON RPC protocol works reliably
   - Google Cloud authentication
   - Supports distributed deployment

4. **Cloud Run Deployment**
   - Automated builds via Cloud Build
   - Auto-scaling containers
   - Endpoint URLs automatically discovered

5. **Streamlit UI**
   - Professional dark theme
   - Chat-first interface
   - Mode indicators (Live/Demo)
   - Secrets management for Streamlit Cloud

### ‚ö†Ô∏è Demo-Only Features

1. **Incident Response**
   - All SOAR actions simulated
   - Cases stored in-memory (lost on restart)
   - No real blocking/isolation/disabling

2. **BigQuery Storage**
   - Works in Cloud Run
   - Disabled in Streamlit Cloud
   - No cross-session persistence in demos

### ‚ùå Not Working / Not Implemented

1. **Vertex AI Agent Registry**
   - All methods are placeholders
   - No actual registration/discovery
   - Uses .env.agents file workaround

2. **A2A Client Discovery**
   - `_resolve_agent_endpoint()` returns None
   - Must provide explicit endpoints

3. **Multi-Indicator Analysis**
   - Only first indicator extracted
   - Bulk analysis not supported

4. **Advanced GTI MCP Tools**
   - 31+ tools discovered but unused
   - No threat hunting workflows
   - No pivot analysis

---

## Conclusion

This codebase is a **pragmatic, production-oriented implementation** that prioritizes:
- ‚úÖ Reliability (direct calling over LLM routing)
- ‚úÖ Speed (bypassing LLM when possible)
- ‚úÖ Cost efficiency (fewer API calls)
- ‚úÖ Graceful degradation (demo mode fallbacks)

However, it **differs significantly from what pure ADK architecture** might suggest:
- Agents are created but routing is mostly manual
- MCP tools are called directly, not via LLM
- Service discovery uses files, not Vertex AI registry
- Some components are incomplete placeholders

**This is not necessarily a problem** - it's a conscious design tradeoff. But it's important to understand when:
- Enhancing features (should you follow the current pattern or move toward full ADK?)
- Debugging issues (the ADK agent might not be involved where you think)
- Planning future work (real registry? real SOAR? more LLM usage?)

---

**End of Analysis**

This document should be updated as the codebase evolves. Key areas to watch:
- If Vertex AI Agent Registry is implemented
- If Chronicle SOAR MCP server becomes available
- If agents shift to more LLM-based routing
- If multi-indicator support is added
